{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0NMsD92XVCV"
      },
      "source": [
        "# 4. Optimizers\n",
        "\n",
        "## Introduction to Gradient-descent Optimizers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfZO5-vqXVCZ"
      },
      "source": [
        "### Model: 1 Hidden Layer Feedforward Neural Network (ReLU Activation)\n",
        "<img src=\"https://raw.githubusercontent.com/SanVik2000/EE5179-Final/main/Tutorial-3/NN.png\" alt=\"deeplearningwizard\" style=\"width: 900px;\"/>\n",
        "\n",
        "In this assignment, we are going to train a MLP model (developed using Pytorch) using different Optimization algorithms that have been already discussed in class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "2IZXty6_XVCa",
        "outputId": "872557d8-c7ea-4ee5-871b-692ae30f9acb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: 500. Loss: 0.3460221588611603. Accuracy: 91.5\n",
            "Iteration: 1000. Loss: 0.21451084315776825. Accuracy: 92.47000122070312\n",
            "Iteration: 1500. Loss: 0.1919996440410614. Accuracy: 93.87000274658203\n",
            "Iteration: 2000. Loss: 0.17153751850128174. Accuracy: 94.47000122070312\n",
            "Iteration: 2500. Loss: 0.11251085251569748. Accuracy: 95.16999816894531\n",
            "Iteration: 3000. Loss: 0.1736811101436615. Accuracy: 95.5999984741211\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as dsets\n",
        "\n",
        "# Set seed\n",
        "torch.manual_seed(0)\n",
        "\n",
        "\n",
        "'''\n",
        "STEP 1: LOADING DATASET\n",
        "'''\n",
        "\n",
        "train_dataset = dsets.MNIST(root='./data', train=True, transform=transforms.ToTensor(), download=True)\n",
        "test_dataset = dsets.MNIST(root='./data', train=False, transform=transforms.ToTensor())\n",
        "\n",
        "'''\n",
        "STEP 2: MAKING DATASET ITERABLE\n",
        "'''\n",
        "\n",
        "batch_size = 100\n",
        "n_iters = 3000\n",
        "num_epochs = n_iters / (len(train_dataset) / batch_size)\n",
        "num_epochs = int(num_epochs)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "'''\n",
        "STEP 3: CREATE MODEL CLASS\n",
        "'''\n",
        "class FeedforwardNeuralNetModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(FeedforwardNeuralNetModel, self).__init__()\n",
        "        # Linear function\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim) \n",
        "        # Non-linearity\n",
        "        self.relu = nn.ReLU()\n",
        "        # Linear function (readout)\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)  \n",
        "    \n",
        "    def forward(self, x):\n",
        "        ### START CODE HERE ###\n",
        "        # Linear function\n",
        "        z1 = self.fc1(x)\n",
        "        # Non-linearity\n",
        "        a1 = self.relu(z1)\n",
        "        # Linear function (readout)\n",
        "        out = self.fc2(a1)\n",
        "        ### END CODE HERE ###\n",
        "        return out\n",
        "'''\n",
        "STEP 4: INSTANTIATE MODEL CLASS\n",
        "'''\n",
        "input_dim = 28*28\n",
        "hidden_dim = 100\n",
        "output_dim = 10\n",
        "\n",
        "model = FeedforwardNeuralNetModel(input_dim, hidden_dim, output_dim)\n",
        "\n",
        "'''\n",
        "STEP 5: INSTANTIATE LOSS CLASS\n",
        "'''\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "'''\n",
        "STEP 6: INSTANTIATE OPTIMIZER CLASS\n",
        "'''\n",
        "learning_rate = 0.1\n",
        "\n",
        "### START CODE HERE ###\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
        "### END CODE HERE ###\n",
        "\n",
        "'''\n",
        "STEP 7: TRAIN THE MODEL\n",
        "'''\n",
        "iter = 0\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        # Load images as Variable\n",
        "        images = images.view(-1, 28*28).requires_grad_()\n",
        "        \n",
        "        # Clear gradients w.r.t. parameters\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Forward pass to get output/logits\n",
        "        outputs = model(images)\n",
        "        \n",
        "        # Calculate Loss: softmax --> cross entropy loss\n",
        "        loss = criterion(outputs, labels)\n",
        "        \n",
        "        # Getting gradients w.r.t. parameters\n",
        "        loss.backward()\n",
        "        \n",
        "        # Updating parameters\n",
        "        optimizer.step()\n",
        "        \n",
        "        iter += 1\n",
        "        \n",
        "        if iter % 500 == 0:\n",
        "            # Calculate Accuracy         \n",
        "            correct = 0\n",
        "            total = 0\n",
        "            # Iterate through test dataset\n",
        "            for images, labels in test_loader:\n",
        "                # Load images to a Torch Variable\n",
        "                images = images.view(-1, 28*28)\n",
        "                \n",
        "                # Forward pass only to get logits/output\n",
        "                outputs = model(images)\n",
        "                \n",
        "                # Get predictions from the maximum value\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                \n",
        "                # Total number of labels\n",
        "                total += labels.size(0)\n",
        "                \n",
        "                # Total correct predictions\n",
        "                correct += (predicted == labels).sum()\n",
        "            \n",
        "            accuracy = 100 * correct / total\n",
        "            \n",
        "            # Print Loss\n",
        "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MxURyoZBXVCc"
      },
      "source": [
        "### Optimization Process \n",
        "\n",
        "`parameters = parameters - learning_rate * parameters_gradients`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THpcpkXRXVCd"
      },
      "source": [
        "### Mathematical Interpretation of Gradient Descent\n",
        "- Model's parameters: $\\theta \\in â„^d$\n",
        "- Loss function: $J(\\theta)$\n",
        "- Gradient w.r.t. parameters: $ \\nabla J(\\theta)$\n",
        "- Learning rate: $\\eta$\n",
        "- Batch Gradient descent: $\\theta = \\theta - \\eta \\cdot  \\nabla J(\\theta)$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJLCOTK0XVCe"
      },
      "source": [
        "## Optimization Algorithm 1: Batch Gradient Descent\n",
        "- What we've covered so far: batch gradient descent\n",
        "    - $\\theta = \\theta - \\eta \\cdot  \\nabla J(\\theta)$\n",
        "- Characteristics\n",
        "    - Compute the gradient of the lost function w.r.t. parameters for the entire training data, $\\nabla J(\\theta)$ \n",
        "    - Use this to update our parameters at every iteration\n",
        "- Problems\n",
        "    - Unable to fit whole datasets in memory \n",
        "    - Computationally slow as we attempt to compute a large Jacobian matrix $\\rightarrow$ first order derivative, $\\nabla J(\\theta)$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "el2oGULyXVCe"
      },
      "source": [
        "## Optimization Algorithm 2: Stochastic Gradient Descent \n",
        "- Modification of batch gradient descent\n",
        "    - $\\theta = \\theta - \\eta \\cdot  \\nabla J(\\theta, x^{i}, y^{i})$\n",
        "- Characteristics\n",
        "    - Compute the gradient of the lost function w.r.t. parameters for the **one set of training sample (1 input and 1 label)**, $\\nabla J(\\theta, x^{i}, y^{i})$\n",
        "    - Use this to update our parameters at every iteration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4-0N5__XVCf"
      },
      "source": [
        "## Optimization Algorithm 3: Mini-batch Gradient Descent\n",
        "- Combination of batch gradient descent & stochastic gradient descent\n",
        "    - $\\theta = \\theta - \\eta \\cdot  \\nabla J(\\theta, x^{i: i+n}, y^{i:i+n})$\n",
        "- Characteristics\n",
        "    - Compute the gradient of the lost function w.r.t. parameters for **n sets of training sample (n input and n label)**, $\\nabla J(\\theta, x^{i: i+n}, y^{i:i+n})$\n",
        "    - Use this to update our parameters at every iteration\n",
        "- This is often called SGD in deep learning frameworks  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ZmG4NYY3XVCg",
        "outputId": "36c2f42e-0b0c-4d83-c739-07bae34ff4c2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: 500. Loss: 0.3460221588611603. Accuracy: 91.5\n",
            "Iteration: 1000. Loss: 0.21451084315776825. Accuracy: 92.47000122070312\n",
            "Iteration: 1500. Loss: 0.1919996440410614. Accuracy: 93.87000274658203\n",
            "Iteration: 2000. Loss: 0.17153751850128174. Accuracy: 94.47000122070312\n",
            "Iteration: 2500. Loss: 0.11251085251569748. Accuracy: 95.16999816894531\n",
            "Iteration: 3000. Loss: 0.1736811101436615. Accuracy: 95.5999984741211\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as dsets\n",
        "\n",
        "# Set seed\n",
        "torch.manual_seed(0)\n",
        "\n",
        "'''\n",
        "STEP 1: LOADING DATASET\n",
        "'''\n",
        "\n",
        "train_dataset = dsets.MNIST(root='./data', train=True, transform=transforms.ToTensor(), download=True)\n",
        "test_dataset = dsets.MNIST(root='./data', train=False, transform=transforms.ToTensor())\n",
        "\n",
        "'''\n",
        "STEP 2: MAKING DATASET ITERABLE\n",
        "'''\n",
        "\n",
        "batch_size = 100\n",
        "n_iters = 3000\n",
        "num_epochs = n_iters / (len(train_dataset) / batch_size)\n",
        "num_epochs = int(num_epochs)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "'''\n",
        "STEP 3: CREATE MODEL CLASS\n",
        "'''\n",
        "class FeedforwardNeuralNetModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(FeedforwardNeuralNetModel, self).__init__()\n",
        "        # Linear function\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim) \n",
        "        # Non-linearity\n",
        "        self.relu = nn.ReLU()\n",
        "        # Linear function (readout)\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)  \n",
        "    \n",
        "    def forward(self, x):\n",
        "        ### START CODE HERE ###\n",
        "        # Linear function\n",
        "        z1 = self.fc1(x)\n",
        "        # Non-linearity\n",
        "        a1 = self.relu(z1)\n",
        "        # Linear function (readout)\n",
        "        out = self.fc2(a1)\n",
        "        ### END CODE HERE ###\n",
        "        return out\n",
        "'''\n",
        "STEP 4: INSTANTIATE MODEL CLASS\n",
        "'''\n",
        "input_dim = 28*28\n",
        "hidden_dim = 100\n",
        "output_dim = 10\n",
        "\n",
        "model = FeedforwardNeuralNetModel(input_dim, hidden_dim, output_dim)\n",
        "\n",
        "'''\n",
        "STEP 5: INSTANTIATE LOSS CLASS\n",
        "'''\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "'''\n",
        "STEP 6: INSTANTIATE OPTIMIZER CLASS\n",
        "'''\n",
        "learning_rate = 0.1\n",
        "\n",
        "### START CODE HERE ###\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
        "### END CODE HERE ###\n",
        "\n",
        "'''\n",
        "STEP 7: TRAIN THE MODEL\n",
        "'''\n",
        "iter = 0\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        # Load images as Variable\n",
        "        images = images.view(-1, 28*28).requires_grad_()\n",
        "        \n",
        "        # Clear gradients w.r.t. parameters\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Forward pass to get output/logits\n",
        "        outputs = model(images)\n",
        "        \n",
        "        # Calculate Loss: softmax --> cross entropy loss\n",
        "        loss = criterion(outputs, labels)\n",
        "        \n",
        "        # Getting gradients w.r.t. parameters\n",
        "        loss.backward()\n",
        "        \n",
        "        # Updating parameters\n",
        "        optimizer.step()\n",
        "        \n",
        "        iter += 1\n",
        "        \n",
        "        if iter % 500 == 0:\n",
        "            # Calculate Accuracy         \n",
        "            correct = 0\n",
        "            total = 0\n",
        "            # Iterate through test dataset\n",
        "            for images, labels in test_loader:\n",
        "                # Load images to a Torch Variable\n",
        "                images = images.view(-1, 28*28).requires_grad_()\n",
        "                \n",
        "                # Forward pass only to get logits/output\n",
        "                outputs = model(images)\n",
        "                \n",
        "                # Get predictions from the maximum value\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                \n",
        "                # Total number of labels\n",
        "                total += labels.size(0)\n",
        "                \n",
        "                # Total correct predictions\n",
        "                correct += (predicted == labels).sum()\n",
        "            \n",
        "            accuracy = 100 * correct / total\n",
        "            \n",
        "            # Print Loss\n",
        "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-qLGENBFXVCh"
      },
      "source": [
        "## Optimization Algorithm 4: SGD Momentum\n",
        "- Modification of SGD\n",
        "    - $v_t = \\gamma v_{t-1} + \\eta \\cdot  \\nabla J(\\theta, x^{i: i+n}, y^{i:i+n})$\n",
        "    - $\\theta = \\theta - v_t$\n",
        "- Characteristics\n",
        "    - Compute the gradient of the lost function w.r.t. parameters for **n sets of training sample (n input and n label)**, $\\nabla J(\\theta, x^{i: i+n}, y^{i:i+n})$\n",
        "    - Use this to add to the previous update vector $v_{t-1}$\n",
        "    - Momentum, usually set to $\\gamma = 0.9$\n",
        "    - Parameters updated with update vector, $v_t$ that incorporates previous update vector\n",
        "        - $\\gamma v_{t}$ increases if gradient same sign/direction as $v_{t-1}$ \n",
        "            - Gives SGD the push when it is going in the right direction (minimizing loss)\n",
        "            - Accelerated convergence\n",
        "        - $\\gamma v_{t}$ decreases if gradient different sign/direction as $v_{t-1}$\n",
        "            - Dampens SGD when it is going in a different direction\n",
        "            - Lower variation in loss minimization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "TnvFGwh7XVCh",
        "outputId": "83cf9e0f-de6a-4a28-bb06-7a93d5b5a13f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: 500. Loss: 0.10879121720790863. Accuracy: 96.01000213623047\n",
            "Iteration: 1000. Loss: 0.12940317392349243. Accuracy: 96.23999786376953\n",
            "Iteration: 1500. Loss: 0.1231849417090416. Accuracy: 96.44000244140625\n",
            "Iteration: 2000. Loss: 0.04057228937745094. Accuracy: 97.52999877929688\n",
            "Iteration: 2500. Loss: 0.04051990807056427. Accuracy: 97.47000122070312\n",
            "Iteration: 3000. Loss: 0.18660661578178406. Accuracy: 97.62999725341797\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as dsets\n",
        "\n",
        "# Set seed\n",
        "torch.manual_seed(0)\n",
        "\n",
        "'''\n",
        "STEP 1: LOADING DATASET\n",
        "'''\n",
        "\n",
        "train_dataset = dsets.MNIST(root='./data', train=True, transform=transforms.ToTensor(), download=True)\n",
        "test_dataset = dsets.MNIST(root='./data', train=False, transform=transforms.ToTensor())\n",
        "\n",
        "'''\n",
        "STEP 2: MAKING DATASET ITERABLE\n",
        "'''\n",
        "\n",
        "batch_size = 100\n",
        "n_iters = 3000\n",
        "num_epochs = n_iters / (len(train_dataset) / batch_size)\n",
        "num_epochs = int(num_epochs)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "'''\n",
        "STEP 3: CREATE MODEL CLASS\n",
        "'''\n",
        "class FeedforwardNeuralNetModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(FeedforwardNeuralNetModel, self).__init__()\n",
        "        # Linear function\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim) \n",
        "        # Non-linearity\n",
        "        self.relu = nn.ReLU()\n",
        "        # Linear function (readout)\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)  \n",
        "    \n",
        "    def forward(self, x):\n",
        "        ### START CODE HERE ###\n",
        "        # Linear function\n",
        "        z1 = self.fc1(x)\n",
        "        # Non-linearity\n",
        "        a1 = self.relu(z1)\n",
        "        # Linear function (readout)\n",
        "        out = self.fc2(a1)\n",
        "        ### END CODE HERE ###\n",
        "        return out\n",
        "'''\n",
        "STEP 4: INSTANTIATE MODEL CLASS\n",
        "'''\n",
        "input_dim = 28*28\n",
        "hidden_dim = 100\n",
        "output_dim = 10\n",
        "\n",
        "model = FeedforwardNeuralNetModel(input_dim, hidden_dim, output_dim)\n",
        "\n",
        "'''\n",
        "STEP 5: INSTANTIATE LOSS CLASS\n",
        "'''\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "'''\n",
        "STEP 6: INSTANTIATE OPTIMIZER CLASS\n",
        "'''\n",
        "learning_rate = 0.1\n",
        "\n",
        "### START CODE HERE ###\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate, momentum = 0.9)\n",
        "### END CODE HERE ###\n",
        "\n",
        "'''\n",
        "STEP 7: TRAIN THE MODEL\n",
        "'''\n",
        "iter = 0\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        # Load images as Variable\n",
        "        images = images.view(-1, 28*28).requires_grad_()\n",
        "        \n",
        "        # Clear gradients w.r.t. parameters\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Forward pass to get output/logits\n",
        "        outputs = model(images)\n",
        "        \n",
        "        # Calculate Loss: softmax --> cross entropy loss\n",
        "        loss = criterion(outputs, labels)\n",
        "        \n",
        "        # Getting gradients w.r.t. parameters\n",
        "        loss.backward()\n",
        "        \n",
        "        # Updating parameters\n",
        "        optimizer.step()\n",
        "        \n",
        "        iter += 1\n",
        "        \n",
        "        if iter % 500 == 0:\n",
        "            # Calculate Accuracy         \n",
        "            correct = 0\n",
        "            total = 0\n",
        "            # Iterate through test dataset\n",
        "            for images, labels in test_loader:\n",
        "                # Load images to a Torch Variable\n",
        "                images = images.view(-1, 28*28)\n",
        "                \n",
        "                # Forward pass only to get logits/output\n",
        "                outputs = model(images)\n",
        "                \n",
        "                # Get predictions from the maximum value\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                \n",
        "                # Total number of labels\n",
        "                total += labels.size(0)\n",
        "                \n",
        "                # Total correct predictions\n",
        "                correct += (predicted == labels).sum()\n",
        "            \n",
        "            accuracy = 100 * correct / total\n",
        "            \n",
        "            # Print Loss\n",
        "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qiZBIHmOXVCi"
      },
      "source": [
        "## Optimization Algorithm 4: Adam\n",
        "- Adaptive Learning Rates\n",
        "    - $m_t = \\beta_1 m_{t-1} + (1 - \\beta_1)g_t$\n",
        "        - Keeping track of decaying gradient\n",
        "        - Estimate of the mean of gradients\n",
        "    - $v_t = \\beta_2 v_{t-1} + (1 - \\beta_2)g_t^2$\n",
        "        - Keeping track of decaying squared gradient \n",
        "        - Estimate of the variance of gradients\n",
        "    - When $m_t, v_t$ initializes as 0, $m_t, v_t \\rightarrow 0$ initially when decay rates small, $\\beta_1, \\beta_2 \\rightarrow 1$  \n",
        "        - Need to correct this with:\n",
        "        - $ \\hat m_t = \\frac{m_t}{1- \\beta_1}$\n",
        "        - $ \\hat v_t = \\frac{v_t}{1- \\beta_2}$\n",
        "    - $\\theta_{t+1} = \\theta_t - \\frac{\\eta}{\\sqrt{\\hat v_t} + \\epsilon}\\hat m_t$\n",
        "    - Default recommended values\n",
        "        - $\\beta_1 = 0.9$\n",
        "        - $\\beta_2 = 0.999$\n",
        "        - $\\epsilon = 10^{-8}$\n",
        "- Instead of learning rate $\\rightarrow$ equations account for estimates of mean/variance of gradients to determine the next learning rate\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "M6I-dAJrXVCj",
        "outputId": "ebfb60fe-db74-4514-a9e3-e9c90eb02146",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: 500. Loss: 0.2257964313030243. Accuracy: 93.30999755859375\n",
            "Iteration: 1000. Loss: 0.17263264954090118. Accuracy: 94.72000122070312\n",
            "Iteration: 1500. Loss: 0.136827290058136. Accuracy: 95.54000091552734\n",
            "Iteration: 2000. Loss: 0.07791975140571594. Accuracy: 96.41000366210938\n",
            "Iteration: 2500. Loss: 0.07298330217599869. Accuracy: 96.9000015258789\n",
            "Iteration: 3000. Loss: 0.1346699446439743. Accuracy: 97.20999908447266\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as dsets\n",
        "\n",
        "# Set seed\n",
        "torch.manual_seed(0)\n",
        "\n",
        "'''\n",
        "STEP 1: LOADING DATASET\n",
        "'''\n",
        "\n",
        "train_dataset = dsets.MNIST(root='./data', train=True, transform=transforms.ToTensor(), download=True)\n",
        "test_dataset = dsets.MNIST(root='./data', train=False, transform=transforms.ToTensor())\n",
        "\n",
        "'''\n",
        "STEP 2: MAKING DATASET ITERABLE\n",
        "'''\n",
        "\n",
        "batch_size = 100\n",
        "n_iters = 3000\n",
        "num_epochs = n_iters / (len(train_dataset) / batch_size)\n",
        "num_epochs = int(num_epochs)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "'''\n",
        "STEP 3: CREATE MODEL CLASS\n",
        "'''\n",
        "class FeedforwardNeuralNetModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(FeedforwardNeuralNetModel, self).__init__()\n",
        "        # Linear function\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim) \n",
        "        # Non-linearity\n",
        "        self.relu = nn.ReLU()\n",
        "        # Linear function (readout)\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)  \n",
        "    \n",
        "    def forward(self, x):\n",
        "        ### START CODE HERE ###\n",
        "        # Linear function\n",
        "        z1 = self.fc1(x)\n",
        "        # Non-linearity\n",
        "        a1 = self.relu(z1)\n",
        "        # Linear function (readout)\n",
        "        out = self.fc2(a1)\n",
        "        ### END CODE HERE ###\n",
        "        return out\n",
        "'''\n",
        "STEP 4: INSTANTIATE MODEL CLASS\n",
        "'''\n",
        "input_dim = 28*28\n",
        "hidden_dim = 100\n",
        "output_dim = 10\n",
        "\n",
        "model = FeedforwardNeuralNetModel(input_dim, hidden_dim, output_dim)\n",
        "\n",
        "'''\n",
        "STEP 5: INSTANTIATE LOSS CLASS\n",
        "'''\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "'''\n",
        "STEP 6: INSTANTIATE OPTIMIZER CLASS\n",
        "'''\n",
        "# learning_rate = 0.001\n",
        "\n",
        "### START CODE HERE ###\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "### END CODE HERE ###\n",
        "\n",
        "'''\n",
        "STEP 7: TRAIN THE MODEL\n",
        "'''\n",
        "iter = 0\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        # Load images as Variable\n",
        "        images = images.view(-1, 28*28).requires_grad_()\n",
        "        \n",
        "        # Clear gradients w.r.t. parameters\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Forward pass to get output/logits\n",
        "        outputs = model(images)\n",
        "        \n",
        "        # Calculate Loss: softmax --> cross entropy loss\n",
        "        loss = criterion(outputs, labels)\n",
        "        \n",
        "        # Getting gradients w.r.t. parameters\n",
        "        loss.backward()\n",
        "        \n",
        "        # Updating parameters\n",
        "        optimizer.step()\n",
        "        \n",
        "        iter += 1\n",
        "        \n",
        "        if iter % 500 == 0:\n",
        "            # Calculate Accuracy         \n",
        "            correct = 0\n",
        "            total = 0\n",
        "            # Iterate through test dataset\n",
        "            for images, labels in test_loader:\n",
        "                # Load images to a Torch Variable\n",
        "                images = images.view(-1, 28*28)\n",
        "                \n",
        "                # Forward pass only to get logits/output\n",
        "                outputs = model(images)\n",
        "                \n",
        "                # Get predictions from the maximum value\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                \n",
        "                # Total number of labels\n",
        "                total += labels.size(0)\n",
        "                \n",
        "                # Total correct predictions\n",
        "                correct += (predicted == labels).sum()\n",
        "            \n",
        "            accuracy = 100 * correct / total\n",
        "            \n",
        "            # Print Loss\n",
        "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66JXO9n0XVCk"
      },
      "source": [
        "## Other Adaptive Algorithms\n",
        "- Other adaptive algorithms (like Adam, adapting learning rates)\n",
        "    - Adagrad\n",
        "    - Adadelta\n",
        "    - Adamax\n",
        "    - RMSProp"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}