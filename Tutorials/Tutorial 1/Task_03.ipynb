{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sd0950ag4NNZ"
      },
      "source": [
        "### Task 03:\n",
        "\n",
        "In this task, you have to implement the Backpropagation method using Pytorch. This is particularly useful when the hypothesis function contains several weights.\n",
        "\n",
        "**Backpropagation**: Algorithm to caculate gradient for all the weights in the network with several weights. \n",
        "\n",
        "* It uses the `Chain Rule` to calcuate the gradient for multiple nodes at the same time. \n",
        "* In pytorch this is implemented using a `variable` data type and `loss.backward()` method to get the gradients"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2w0MM9204NNh"
      },
      "outputs": [],
      "source": [
        "# import the necessary libraries\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "st0vmvel4NNn"
      },
      "source": [
        "## Preliminaries - Pytorch Basics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3BUgr0pR4NNq",
        "outputId": "aad8f92a-dd48-45f0-9136-0b042836a2c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0., 0., 0., 0., 0.])\n",
            "tensor([1., 1., 1., 1., 1.])\n",
            "tensor([ 0.8607, -1.0401, -0.3258,  0.8999,  0.6351])\n",
            "The size of the to_tensor:  torch.Size([5])\n",
            "The size of the to_tensor:  torch.Size([5])\n"
          ]
        }
      ],
      "source": [
        "# creating a tensor\n",
        "\n",
        "# zero tensor\n",
        "zeros = torch.zeros(5)\n",
        "print(zeros)\n",
        "# ones\n",
        "ones = torch.ones(5)\n",
        "print(ones)\n",
        "# random normal\n",
        "random = torch.randn(5)\n",
        "print(random)\n",
        "\n",
        "\n",
        "# creating tensors from list and/or numpy arrays\n",
        "my_list = [0.0, 1.0, 2.0, 3.0, 4.0]\n",
        "to_tensor = torch.Tensor(my_list)\n",
        "print(\"The size of the to_tensor: \", to_tensor.size())\n",
        "\n",
        "my_array = np.array(my_list) # or\n",
        "to_tensor = torch.tensor(my_array)\n",
        "to_tensor = torch.from_numpy(my_array)\n",
        "print(\"The size of the to_tensor: \", to_tensor.size())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Js4p4btA4VvG",
        "outputId": "98af814d-f39b-43e8-8e13-41ca6f857ce4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 1.6727, -0.3045, -1.9060],\n",
            "        [ 1.4567, -0.6158, -0.0362],\n",
            "        [ 0.3325,  1.5602, -0.6537]])\n",
            "tensor([[[-0.2984,  0.5324, -0.6734],\n",
            "         [-0.2027,  0.2507,  0.0708],\n",
            "         [-0.2023,  0.0649,  0.9411]],\n",
            "\n",
            "        [[ 0.0836, -0.9937, -0.6178],\n",
            "         [ 0.7532,  1.2793, -1.2999],\n",
            "         [-0.8546,  0.0534, -0.9658]],\n",
            "\n",
            "        [[ 0.6575, -2.0642,  1.7661],\n",
            "         [ 0.7572,  0.2299,  1.9890],\n",
            "         [-0.8458, -0.3393, -1.2135]]])\n"
          ]
        }
      ],
      "source": [
        "# multi dimenstional tensors\n",
        "\n",
        "# 2D\n",
        "two_dim = torch.randn((3, 3))\n",
        "print(two_dim)\n",
        "# 3D \n",
        "three_dim = torch.randn((3, 3, 3))\n",
        "print(three_dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lRA0Icia4ani",
        "outputId": "ce1f109f-f302-4575-e7e0-4de2851c352d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([5])\n",
            "torch.Size([3, 3])\n",
            "torch.Size([3, 3, 3])\n",
            "tensor([1.6727, 1.4567, 0.3325])\n",
            "tensor([ 1.6727, -0.3045, -1.9060])\n"
          ]
        }
      ],
      "source": [
        "# tensor shapes and axes\n",
        "\n",
        "print(zeros.shape)\n",
        "print(two_dim.shape)\n",
        "print(three_dim.shape)\n",
        "\n",
        "# zeroth axis - rows\n",
        "print(two_dim[:, 0])\n",
        "# first axis - columns\n",
        "print(two_dim[0, :])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4CFwt4R_4iEq",
        "outputId": "517555a3-d7ba-4c3e-f59d-2acf719c7ded"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 1.6727, -0.3045],\n",
            "        [ 1.4567, -0.6158],\n",
            "        [ 0.3325,  1.5602]])\n",
            "tensor([[ 1.6727, -0.3045, -1.9060],\n",
            "        [ 1.4567, -0.6158, -0.0362]])\n"
          ]
        }
      ],
      "source": [
        "print(two_dim[:, 0:2])\n",
        "print(two_dim[0:2, :])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJA1f5_F5Acn",
        "outputId": "8904174c-839b-4c51-bc02-1da573b3b7a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensor Shape :  torch.Size([2, 3])\n",
            "Resized Tensor Shape :  torch.Size([3, 2])\n",
            "Resized Tensor Shape :  torch.Size([3, 2])\n",
            "Flattened Tensor Shape :  torch.Size([6])\n"
          ]
        }
      ],
      "source": [
        "rand_tensor = torch.randn(2,3)\n",
        "print(\"Tensor Shape : \" , rand_tensor.shape)\n",
        "resized_tensor = rand_tensor.reshape(3,2)\n",
        "print(\"Resized Tensor Shape : \" , resized_tensor.shape) # or\n",
        "resized_tensor = rand_tensor.reshape(3,-1)\n",
        "print(\"Resized Tensor Shape : \" , resized_tensor.shape)\n",
        "flattened_tensor = rand_tensor.reshape(-1)\n",
        "print(\"Flattened Tensor Shape : \" , flattened_tensor.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ismMPyM4NNw"
      },
      "source": [
        "Determine the derivative of $y = 2x^{3} + x$ at $x = 1$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ic-ZkAPC4NNx",
        "outputId": "4dce00b0-a19c-4111-c125-6507aa196188"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Value of Y at x=1 :  tensor(3., grad_fn=<AddBackward0>)\n",
            "Derivative of Y wrt x at x=1 :  tensor(7.)\n"
          ]
        }
      ],
      "source": [
        "x = torch.tensor(1.0, requires_grad = True)\n",
        "y = 2 * (x ** 3) + x\n",
        "y.backward()\n",
        "print(\"Value of Y at x=1 : \" , y)\n",
        "print(\"Derivative of Y wrt x at x=1 : \" , x.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gp-J-JDA4NNy"
      },
      "source": [
        "### Task 03 - a\n",
        "Determine the partial derivative of $y = uv + u^{2}$ at $u=1$ and $v=2$ with respect to $u$ and $v$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "N7ShF6wf4NNz",
        "outputId": "11f04169-4166-4c96-913a-a2301387e8a7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Value of y at u=1, v=2 :  tensor(3., grad_fn=<AddBackward0>)\n",
            "Partial Derivative of y wrt u :  tensor(4.)\n",
            "Partial Derivative of y wrt v :  tensor(1.)\n"
          ]
        }
      ],
      "source": [
        "# YOUR CODE STARTS HERE\n",
        "u = torch.tensor(1.0, requires_grad = True)\n",
        "v = torch.tensor(2.0, requires_grad = True)\n",
        "y = u*v + u**2\n",
        "\n",
        "y.backward()\n",
        "# YOUR CODE ends HERE\n",
        "print(\"Value of y at u=1, v=2 : \" , y)\n",
        "print(\"Partial Derivative of y wrt u : \" , u.grad)\n",
        "print(\"Partial Derivative of y wrt v : \" , v.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29jvHyKE4NN0"
      },
      "source": [
        "#### Hypothesis Function and Loss Function\n",
        "\n",
        "$y = x * w + b$\n",
        "\n",
        "$loss =(\\hat{y}-y)^2$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7dU0O9E4NN0"
      },
      "source": [
        "Let us make use of a randomly-created sample dataset as follows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "6tUXuGub4NN1"
      },
      "outputs": [],
      "source": [
        "#sample-dataset\n",
        "x_data = [1.0, 2.0, 3.0]\n",
        "y_data = [2.0, 4.0, 6.0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BI84dQfW4NN1"
      },
      "source": [
        "## Task: 03 - b\n",
        "Declare pytorch tensors for weight and bias and implement the forward and loss function of our model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "ZQaYqClT4NN1"
      },
      "outputs": [],
      "source": [
        "# Define w = 1 and b = -1 for y = wx + b\n",
        "# Note that w,b are learnable paramteter \n",
        "# i.e., you are going to take the derivative of the tensor(s).\n",
        "# YOUR CODE STARTS HERE\n",
        "w = torch.tensor(1., requires_grad = True)\n",
        "b = torch.tensor(-1., requires_grad = True)\n",
        "# YOUR CODE ENDS HERE\n",
        "\n",
        "assert w.item() == 1\n",
        "assert b.item() == -1\n",
        "assert w.requires_grad == True\n",
        "assert b.requires_grad == True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "zRVK-4tr4NN3"
      },
      "outputs": [],
      "source": [
        "#forward function to calculate y_pred for a given x according to the linear model defined above\n",
        "def forward(w, b, x):\n",
        "    #implement the forward model to compute y_pred as w*x + b\n",
        "    ## YOUR CODE STARTS HERE\n",
        "    return w*x + b\n",
        "\n",
        "    ## YOUR CODE ENDS HERE\n",
        "\n",
        "#loss-function to compute the mean-squared error between y_pred and y_actual\n",
        "def loss(y_pred, y_actual):\n",
        "    #calculate the mean-squared-error between y_pred and y_actual\n",
        "    ## YOUR CODE STARTS HERE\n",
        "    return (y_pred - y_actual)**2\n",
        "\n",
        "    ## YOUR CODE ENDS HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3BAF5aQ4NN3"
      },
      "source": [
        "Calculate $y_{pred}$ for $x=4$ without training the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "EPAgSpvE4NN4"
      },
      "outputs": [],
      "source": [
        "y_pred_without_train = forward(w, b, 4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0BO6QOWq4NN4"
      },
      "source": [
        "Begin Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "VUgLcPrG4NN4",
        "outputId": "7395bea5-a946-4c87-f919-f961627ab976",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 | Loss: 8.035331726074219 | w: 1.361407995223999\n",
            "Epoch: 2 | Loss: 3.9245269298553467 | w: 1.6126642227172852\n",
            "Epoch: 3 | Loss: 1.9271574020385742 | w: 1.7872123718261719\n",
            "Epoch: 4 | Loss: 0.9558445811271667 | w: 1.9083435535430908\n",
            "Epoch: 5 | Loss: 0.48289188742637634 | w: 1.9922776222229004\n",
            "Epoch: 6 | Loss: 0.2521496117115021 | w: 2.0503103733062744\n",
            "Epoch: 7 | Loss: 0.1392294019460678 | w: 2.090308427810669\n",
            "Epoch: 8 | Loss: 0.08369665592908859 | w: 2.1177496910095215\n",
            "Epoch: 9 | Loss: 0.056166838854551315 | w: 2.1364498138427734\n",
            "Epoch: 10 | Loss: 0.04233689606189728 | w: 2.1490650177001953\n"
          ]
        }
      ],
      "source": [
        "# In this method, we learn the dataset multiple times (called epochs)\n",
        "# Each time, the weight (w) gets updates using the graident decent algorithm based on weights of the previous epoch\n",
        "\n",
        "alpha = 0.01 # Let us set learning rate as 0.01\n",
        "weight_list = []\n",
        "loss_list=[]\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(10):\n",
        "    total_loss = 0\n",
        "    count = 0\n",
        "    \n",
        "    for x, y in zip(x_data, y_data):\n",
        "        \n",
        "        #implement forward pass, compute loss and gradients for the weights and update weights\n",
        "        ## YOUR CODE STARTS HERE\n",
        "        y_pred = forward(w, b, x)\n",
        "        current_loss = loss(y_pred, y)\n",
        "        current_loss.backward()\n",
        "        grad = [w.grad, b.grad]\n",
        "        w.data = w.data - alpha*grad[0].item()\n",
        "        b.data = b.data - alpha*grad[1].item()\n",
        "        total_loss += current_loss\n",
        "        ## YOUR CODE ENDS HERE\n",
        "\n",
        "        # Manually zero the gradients after updating weights\n",
        "        w.grad.data.zero_()\n",
        "        b.grad.data.zero_()\n",
        "        count += 1\n",
        "        \n",
        "    avg_mse = total_loss / count        \n",
        "    print(f\"Epoch: {epoch+1} | Loss: {avg_mse.item()} | w: {w.item()}\")\n",
        "    weight_list.append(w)\n",
        "    loss_list.append(avg_mse)  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1vfWzjw14NN5"
      },
      "source": [
        "Calculate $y_{pred}$ for $x=4$ after training the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "KiQzppwU4NN6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7477cc44-b789-465f-eada-c635f22d2e3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Actual Y Value for x=4 : 8\n",
            "Predicted Y Value before training :  3.0\n",
            "Predicted Y Value after training :  8.151883125305176\n"
          ]
        }
      ],
      "source": [
        "y_pred_with_train = forward(w, b, 4)\n",
        "\n",
        "print(\"Actual Y Value for x=4 : 8\")\n",
        "print(\"Predicted Y Value before training : \" , y_pred_without_train.item())\n",
        "print(\"Predicted Y Value after training : \" , y_pred_with_train.item())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mfsMg3xe4NN6"
      },
      "source": [
        "## Task: 03 - c\n",
        "Repeat **Task:03 - b** for the quadratic model defined below"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvN4ZfIt4NN6"
      },
      "source": [
        "#### Using backward propagation for quadratic model\n",
        "\n",
        "$\\hat{y} = x^2*w_{2} + x*w_{1}$\n",
        "\n",
        "$loss = (\\hat{y}-y)^2$\n",
        "\n",
        "* Using Dummy values of x and y\n",
        "\n",
        "`x = 1,2,3,4,5`\n",
        "`y = 1,6,15,28,45`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "pKxTLgYc4NN7"
      },
      "outputs": [],
      "source": [
        "x_data = [1.0, 2.0, 3.0, 4.0, 5.0]\n",
        "y_data = [1.0, 6.0, 15.0, 28, 45]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "0tvvovED4NN7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "13948672-72ad-4c46-9f8b-93fa3129b6de"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# Visualize the given dataset\n",
        "plt.plot(x_data,y_data)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "nodeeWDX4NN7"
      },
      "outputs": [],
      "source": [
        "# Initialize w2 and w1 with randon values\n",
        "w_1 = torch.tensor([1.0], requires_grad=True)\n",
        "w_2 = torch.tensor([1.0], requires_grad=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "4DZxKgss4NN7"
      },
      "outputs": [],
      "source": [
        "#wuadratic-forward function to calculate y_pred for a given x according to the quadratic model defined above\n",
        "def quad_forward(w1, w2, x):\n",
        "    #implement the forward model to compute y_pred as w1*x + w2*(x^2)\n",
        "    ## YOUR CODE STARTS HERE\n",
        "    return w1*x + w2*(x**2)\n",
        "\n",
        "    ## YOUR CODE ENDS HERE\n",
        "\n",
        "#loss-function to compute the mean-squared error between y_pred and y_actual\n",
        "def loss(y_pred, y_actual):\n",
        "    #calculate the mean-squared-error between y_pred and y_actual\n",
        "    ## YOUR CODE STARTS HERE\n",
        "    return (y_pred - y_actual)**2\n",
        "\n",
        "    ## YOUR CODE ENDS HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mKlMp8sW4NN8"
      },
      "source": [
        "Calculate $y_{pred}$ for $x=6$ without training the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "YiVbYy1P4NN8"
      },
      "outputs": [],
      "source": [
        "y_pred_without_train = quad_forward(w_1, w_2, 6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfUGvo2F4NN8"
      },
      "source": [
        "Begin Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "04CX2FJX4NN9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7bba46ec-3cef-4f1e-fdc0-ab7e4f66a91d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 | Loss: 407.6036682128906 | w: 2.1490650177001953\n",
            "Epoch: 2 | Loss: 319.4317932128906 | w: 2.1490650177001953\n",
            "Epoch: 3 | Loss: 253.5703582763672 | w: 2.1490650177001953\n",
            "Epoch: 4 | Loss: 204.37718200683594 | w: 2.1490650177001953\n",
            "Epoch: 5 | Loss: 167.63479614257812 | w: 2.1490650177001953\n",
            "Epoch: 6 | Loss: 140.19107055664062 | w: 2.1490650177001953\n",
            "Epoch: 7 | Loss: 119.6903305053711 | w: 2.1490650177001953\n",
            "Epoch: 8 | Loss: 104.37223052978516 | w: 2.1490650177001953\n",
            "Epoch: 9 | Loss: 92.92167663574219 | w: 2.1490650177001953\n",
            "Epoch: 10 | Loss: 84.35618591308594 | w: 2.1490650177001953\n",
            "Epoch: 11 | Loss: 77.9421157836914 | w: 2.1490650177001953\n",
            "Epoch: 12 | Loss: 73.13150024414062 | w: 2.1490650177001953\n",
            "Epoch: 13 | Loss: 69.51544189453125 | w: 2.1490650177001953\n",
            "Epoch: 14 | Loss: 66.7886734008789 | w: 2.1490650177001953\n",
            "Epoch: 15 | Loss: 64.72352600097656 | w: 2.1490650177001953\n",
            "Epoch: 16 | Loss: 63.15018844604492 | w: 2.1490650177001953\n",
            "Epoch: 17 | Loss: 61.9421272277832 | w: 2.1490650177001953\n",
            "Epoch: 18 | Loss: 61.00507736206055 | w: 2.1490650177001953\n",
            "Epoch: 19 | Loss: 60.26884078979492 | w: 2.1490650177001953\n",
            "Epoch: 20 | Loss: 59.68120193481445 | w: 2.1490650177001953\n",
            "Epoch: 21 | Loss: 59.203369140625 | w: 2.1490650177001953\n",
            "Epoch: 22 | Loss: 58.80651092529297 | w: 2.1490650177001953\n",
            "Epoch: 23 | Loss: 58.469268798828125 | w: 2.1490650177001953\n",
            "Epoch: 24 | Loss: 58.175819396972656 | w: 2.1490650177001953\n",
            "Epoch: 25 | Loss: 57.91440963745117 | w: 2.1490650177001953\n",
            "Epoch: 26 | Loss: 57.67646408081055 | w: 2.1490650177001953\n",
            "Epoch: 27 | Loss: 57.45561981201172 | w: 2.1490650177001953\n",
            "Epoch: 28 | Loss: 57.247169494628906 | w: 2.1490650177001953\n",
            "Epoch: 29 | Loss: 57.047752380371094 | w: 2.1490650177001953\n",
            "Epoch: 30 | Loss: 56.854820251464844 | w: 2.1490650177001953\n",
            "Epoch: 31 | Loss: 56.66658401489258 | w: 2.1490650177001953\n",
            "Epoch: 32 | Loss: 56.48176193237305 | w: 2.1490650177001953\n",
            "Epoch: 33 | Loss: 56.29937744140625 | w: 2.1490650177001953\n",
            "Epoch: 34 | Loss: 56.118804931640625 | w: 2.1490650177001953\n",
            "Epoch: 35 | Loss: 55.93956756591797 | w: 2.1490650177001953\n",
            "Epoch: 36 | Loss: 55.7613525390625 | w: 2.1490650177001953\n",
            "Epoch: 37 | Loss: 55.58392333984375 | w: 2.1490650177001953\n",
            "Epoch: 38 | Loss: 55.40714645385742 | w: 2.1490650177001953\n",
            "Epoch: 39 | Loss: 55.23090744018555 | w: 2.1490650177001953\n",
            "Epoch: 40 | Loss: 55.0551872253418 | w: 2.1490650177001953\n",
            "Epoch: 41 | Loss: 54.87995147705078 | w: 2.1490650177001953\n",
            "Epoch: 42 | Loss: 54.70519256591797 | w: 2.1490650177001953\n",
            "Epoch: 43 | Loss: 54.5308723449707 | w: 2.1490650177001953\n",
            "Epoch: 44 | Loss: 54.35703659057617 | w: 2.1490650177001953\n",
            "Epoch: 45 | Loss: 54.1837158203125 | w: 2.1490650177001953\n",
            "Epoch: 46 | Loss: 54.01088333129883 | w: 2.1490650177001953\n",
            "Epoch: 47 | Loss: 53.83856201171875 | w: 2.1490650177001953\n",
            "Epoch: 48 | Loss: 53.66679000854492 | w: 2.1490650177001953\n",
            "Epoch: 49 | Loss: 53.49555587768555 | w: 2.1490650177001953\n",
            "Epoch: 50 | Loss: 53.32490921020508 | w: 2.1490650177001953\n",
            "Epoch: 51 | Loss: 53.15483856201172 | w: 2.1490650177001953\n",
            "Epoch: 52 | Loss: 52.98534393310547 | w: 2.1490650177001953\n",
            "Epoch: 53 | Loss: 52.81645584106445 | w: 2.1490650177001953\n",
            "Epoch: 54 | Loss: 52.6481819152832 | w: 2.1490650177001953\n",
            "Epoch: 55 | Loss: 52.48052215576172 | w: 2.1490650177001953\n",
            "Epoch: 56 | Loss: 52.313499450683594 | w: 2.1490650177001953\n",
            "Epoch: 57 | Loss: 52.147071838378906 | w: 2.1490650177001953\n",
            "Epoch: 58 | Loss: 51.981300354003906 | w: 2.1490650177001953\n",
            "Epoch: 59 | Loss: 51.81618118286133 | w: 2.1490650177001953\n",
            "Epoch: 60 | Loss: 51.65167236328125 | w: 2.1490650177001953\n",
            "Epoch: 61 | Loss: 51.487831115722656 | w: 2.1490650177001953\n",
            "Epoch: 62 | Loss: 51.324607849121094 | w: 2.1490650177001953\n",
            "Epoch: 63 | Loss: 51.16205596923828 | w: 2.1490650177001953\n",
            "Epoch: 64 | Loss: 51.000118255615234 | w: 2.1490650177001953\n",
            "Epoch: 65 | Loss: 50.838844299316406 | w: 2.1490650177001953\n",
            "Epoch: 66 | Loss: 50.67821502685547 | w: 2.1490650177001953\n",
            "Epoch: 67 | Loss: 50.518211364746094 | w: 2.1490650177001953\n",
            "Epoch: 68 | Loss: 50.358863830566406 | w: 2.1490650177001953\n",
            "Epoch: 69 | Loss: 50.20015335083008 | w: 2.1490650177001953\n",
            "Epoch: 70 | Loss: 50.042091369628906 | w: 2.1490650177001953\n",
            "Epoch: 71 | Loss: 49.88465881347656 | w: 2.1490650177001953\n",
            "Epoch: 72 | Loss: 49.727874755859375 | w: 2.1490650177001953\n",
            "Epoch: 73 | Loss: 49.57170486450195 | w: 2.1490650177001953\n",
            "Epoch: 74 | Loss: 49.416194915771484 | w: 2.1490650177001953\n",
            "Epoch: 75 | Loss: 49.26130294799805 | w: 2.1490650177001953\n",
            "Epoch: 76 | Loss: 49.10703659057617 | w: 2.1490650177001953\n",
            "Epoch: 77 | Loss: 48.95338821411133 | w: 2.1490650177001953\n",
            "Epoch: 78 | Loss: 48.800384521484375 | w: 2.1490650177001953\n",
            "Epoch: 79 | Loss: 48.64799499511719 | w: 2.1490650177001953\n",
            "Epoch: 80 | Loss: 48.49622344970703 | w: 2.1490650177001953\n",
            "Epoch: 81 | Loss: 48.3450813293457 | w: 2.1490650177001953\n",
            "Epoch: 82 | Loss: 48.194557189941406 | w: 2.1490650177001953\n",
            "Epoch: 83 | Loss: 48.044647216796875 | w: 2.1490650177001953\n",
            "Epoch: 84 | Loss: 47.89533233642578 | w: 2.1490650177001953\n",
            "Epoch: 85 | Loss: 47.74663543701172 | w: 2.1490650177001953\n",
            "Epoch: 86 | Loss: 47.59854507446289 | w: 2.1490650177001953\n",
            "Epoch: 87 | Loss: 47.451072692871094 | w: 2.1490650177001953\n",
            "Epoch: 88 | Loss: 47.3041877746582 | w: 2.1490650177001953\n",
            "Epoch: 89 | Loss: 47.15789031982422 | w: 2.1490650177001953\n",
            "Epoch: 90 | Loss: 47.01221466064453 | w: 2.1490650177001953\n",
            "Epoch: 91 | Loss: 46.86711502075195 | w: 2.1490650177001953\n",
            "Epoch: 92 | Loss: 46.72262191772461 | w: 2.1490650177001953\n",
            "Epoch: 93 | Loss: 46.578712463378906 | w: 2.1490650177001953\n",
            "Epoch: 94 | Loss: 46.435386657714844 | w: 2.1490650177001953\n",
            "Epoch: 95 | Loss: 46.29265213012695 | w: 2.1490650177001953\n",
            "Epoch: 96 | Loss: 46.15050506591797 | w: 2.1490650177001953\n",
            "Epoch: 97 | Loss: 46.00894546508789 | w: 2.1490650177001953\n",
            "Epoch: 98 | Loss: 45.867950439453125 | w: 2.1490650177001953\n",
            "Epoch: 99 | Loss: 45.727535247802734 | w: 2.1490650177001953\n",
            "Epoch: 100 | Loss: 45.58769226074219 | w: 2.1490650177001953\n"
          ]
        }
      ],
      "source": [
        "# In this method, we learn the dataset multiple times (called epochs)\n",
        "# Each time, the weight (w) gets updates using the graident decent algorithm based on weights of the previous epoch\n",
        "\n",
        "alpha = 0.0012 # Let us set learning rate as 0.01\n",
        "weight_list = []\n",
        "loss_list=[]\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(100):\n",
        "    total_loss = 0\n",
        "    count = 0\n",
        "    \n",
        "    for x, y in zip(x_data, y_data):\n",
        "        \n",
        "        #implement forward pass, compute loss and gradients for the weights and update weights\n",
        "        ## YOUR CODE STARTS HERE\n",
        "        y_pred = forward(w_1, w_2, x)\n",
        "        current_loss = loss(y_pred, y)\n",
        "        current_loss.backward()\n",
        "        grad = [w_1.grad, w_2.grad]\n",
        "        w_1.data = w_1.data - alpha*grad[0].item()\n",
        "        w_2.data = w_2.data - alpha*grad[1].item()\n",
        "        total_loss += current_loss\n",
        "        ## YOUR CODE ENDS HERE\n",
        "        \n",
        "        # Manually zero the gradients after updating weights\n",
        "        w_1.grad.data.zero_()\n",
        "        w_2.grad.data.zero_()\n",
        "        \n",
        "        count += 1\n",
        "        \n",
        "    avg_mse = total_loss / count        \n",
        "    print(f\"Epoch: {epoch+1} | Loss: {avg_mse.item()} | w: {w.item()}\")\n",
        "    weight_list.append(w)\n",
        "    loss_list.append(avg_mse)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCxaFfeb4NN9"
      },
      "source": [
        "Calculate $y_{pred}$ for $x=6$ after training the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "alIVupLO4NN9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "277e7faf-4186-4449-a8b2-8885743efa01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Actual Y Value for x=4 : 66\n",
            "Predicted Y Value before training :  42.0\n",
            "Predicted Y Value after training :  43.666015625\n"
          ]
        }
      ],
      "source": [
        "y_pred_with_train = forward(w_1, w_2, 6)\n",
        "\n",
        "print(\"Actual Y Value for x=4 : 66\")\n",
        "print(\"Predicted Y Value before training : \" , y_pred_without_train.item())\n",
        "print(\"Predicted Y Value after training : \" , y_pred_with_train.item())"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Task_03.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}